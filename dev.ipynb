{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fcb6a35",
   "metadata": {},
   "source": [
    "# TODO: Review commented out parameters in lvae TopDownLayer and reimplement\n",
    "1) TODO when we meet in person: Review if I implemented head_z_dims correctly (done)\n",
    "2) Unknown params:\n",
    "- use_mode (never used?) # ok to remove. -> depends on mode_layers # remove also this (it's for sampling that we don't need)\n",
    "- force_constant_output (never used?) # dunno (removed)\n",
    "- difference between use_uncond_mode and inference_mode?\n",
    "    - in TopDownLayer.forward the merge layer is commented out, so use_uncond_mode is uneffective. Why? Does it matter?\n",
    "        - TODO: YES. This is one of the parts to ablate. It is currently disabled because it lead to better results, we need to make it switchable for-layer.\n",
    "          - there is self.use_uncond_mode_at that can be used for this.\n",
    "        - This has been renamed to use_skip_connections (done)\n",
    "3) There are A LOT of places when the networks are on the wrong device.\n",
    "    - Should be fixed (done)\n",
    "4) Do we need to ablate bu_values or skip_connections in TopDownLayer?\n",
    "    - both\n",
    "5) Check if the KL handling in inference is correct (MixtureStochasticConvBlock.forward when label is None) (done)\n",
    "\n",
    "- Entropy should be removed (done)\n",
    "- \"pi\" became \"class_probabilities\" (done)\n",
    "\n",
    "\n",
    "-  !!!! Threshold: Should be added as a scheduler and decrease donly in semisupervised mode (see trainer.py line 284 in Sheida's repo)\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a866f92",
   "metadata": {},
   "source": [
    "## TODOS\n",
    "- Log validation losses too\n",
    "- [ ] check data_mean and data_std are actually needed in this model (disabled for now)\n",
    "- [ ] add .get_data_statistics() to the datamodule\n",
    "- [ ] Reimplement data_mean and data_std handling where appropriate\n",
    "- Mask and Label size depend on the \"wait_count\" (i.e., patience) of EarlyStopping in the original training script. Is that correct?\n",
    "- Also ModeSwitchCallback depends on EarlyStopping patience.\n",
    "- I noticed that it actually resets the wait_count to 0, need to find a way to do it.\n",
    "- ...shouldn't we just do it when the EarlyStopping triggers?\n",
    "- Implement WandbLogger callback and eventually tensorboard logging too\n",
    "- Implement dataset\n",
    "- Train :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb7981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eps_seg.config import LVAEConfig\n",
    "from eps_seg.models import LVAEModel\n",
    "import torch \n",
    "\n",
    "config = LVAEConfig()\n",
    "print(config)\n",
    "\n",
    "x = torch.randn(2, 1, 64, 64)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = x.to(device)\n",
    "model = LVAEModel(config).to(device)\n",
    "model.eval()\n",
    "model(x) # This DOES NOT normalize data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7393fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name=(FieldInfo(annotation=NoneType, required=False, default='eps_seg_default', description='Name of the model'),) lr=3e-05 max_epochs=1000 batch_size=256 amp=True gradient_scale=256 max_grad_norm=1.0 alpha=1.0 beta=0.1 gamma=1.0 use_wandb=True initial_label_size=1 final_label_size=10 initial_mask_size=1 final_mask_size=10 step_interval=20\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "from eps_seg.config.train import TrainConfig\n",
    "from eps_seg.train.callbacks import ModelCheckpoint, WandbLogger, EarlyStoppingWithWaitCount, ThresholdSchedulerCallback, MaskLabelSizeSchedulerCallback, ModeSwitchCallback, RadiusIncreaseCallback\n",
    "\n",
    "train_config = TrainConfig()\n",
    "print(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f8f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "\n",
    "# Fixme: Use lightning built-ins\n",
    "callbacks.append(ModelCheckpoint(monitor=\"val_total\", save_top_k=1))\n",
    "callbacks.append(EarlyStoppingWithWaitCount(monitor=\"val_total\", patience=50))\n",
    "# FIXME: Use correct model checkpoint folder\n",
    "callbacks.append(ModeSwitchCallback(dirpath=model.train_cfg.model_name,\n",
    "                                    patience_threshold=50))\n",
    "callbacks.append(RadiusIncreaseCallback(dirpath=model.train_cfg.model_name,\n",
    "                                       patience_threshold=50))\n",
    "\n",
    "\n",
    "# TODO: Write a Callback not a Scheduler\n",
    "if train_config.initial_label_size != train_config.final_label_size:\n",
    "    callbacks.append(\n",
    "        MaskLabelSizeSchedulerCallback(\n",
    "            initial_label=train_config.initial_label_size,\n",
    "            final_label=train_config.final_label_size,\n",
    "            initial_mask=train_config.initial_mask_size,\n",
    "            final_mask=train_config.final_mask_size,\n",
    "            step_interval=train_config.step_interval,\n",
    "            use_patience=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# TODO: Mode Switch Callback before this\n",
    "\n",
    "callbacks.append(ThresholdSchedulerCallback(\n",
    "    initial_threshold=train_config.initial_threshold,\n",
    "    max_threshold=train_config.max_threshold,\n",
    "    step_size=train_config.threshold_step,\n",
    "    mode=\"semisupervised\"\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    devices=1, \n",
    "    max_epochs=train_config.max_epochs,\n",
    "    logger=WandbLogger(project=train_config.model_name) if train_config.use_wandb else None,\n",
    "    callbacks=callbacks,\n",
    "    precision = \"16-mixed\" if train_config.amp else 32,\n",
    "    gradient_clip_val=train_config.max_grad_norm, \n",
    "    log_every_n_steps=train_config.log_every_n_steps,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdea8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eps_seg.train.optimizers import LabelSizeScheduler\n",
    "label_size_scheduler = LabelSizeScheduler(\n",
    "    initial_size=train_config.initial_label_size,\n",
    "    final_size=train_config.final_label_size,\n",
    "    step_interval=train_config.step_interval,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1cbfcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for s in range(100):\n",
    "    print(label_size_scheduler.get_label_size(current_step=s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eps-Seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
